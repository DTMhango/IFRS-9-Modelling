{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IFRS 9 IMPAIRMENT MODEL ~~ BY DANIEL TIWONGE MHANGO (Development in Progress)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PROBABILITY OF DEFAULT MODEL"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cells below contain the code used to generate the probabilities of default from the historical loan data (recommended 5 years of data).\n",
    "These are derived from transition matrices that track the transition of balances between the various IFRS 9 stages (Stage 1, Stage 2, Stage 3).\n",
    "We have assumed that loans cannot transition out of defualt (cannot cure) and thus created an absorbing state in the matrices. .\n",
    "FLI adjustments are to be applied to the pds in the MacroEconomic Module to obtain the Final pds. Macroeconomic data is obtained from the IMF using the web API. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "start_time = time.time()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPORT DATA FILES\n",
    "Import Data; Declare Valuation/Reporting Date; Declare Periodicity; Declare Staging Criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare path to data and read in data file\n",
    "path_read = 'Input Files/'\n",
    "path_write = 'Output Files/'\n",
    "\n",
    "# import data and create backup of original data files\n",
    "pd_data_backup = pd.read_csv(path_read + 'Example PD Data.csv')\n",
    "pd_data = pd_data_backup.copy()\n",
    "\n",
    "rr_data_backup = pd.read_csv(path_read + 'Example Recoveries Data.csv')\n",
    "recoveries_data = rr_data_backup.copy()\n",
    "\n",
    "# Define keys and values for pd headers_mapping dictionary\n",
    "pd_keys = ['data_date', 'client_id', 'account_number', 'loan_segment', 'eir', 'risk_status', 'out_bal', 'days_past_due']\n",
    "pd_values = ['DATA_DATE', 'CLIENT_ID', 'ACCOUNT_NUMBER', 'LOAN_SEGMENT', 'EIR','RISK_STATUS', 'OUT_BAL', 'DAYS_PAST_DUE']\n",
    "# Map the column headers (values) of the dataframe to the corresponding keys\n",
    "pd_column_headers_dict = {k: v for k, v in zip(pd_keys, pd_values)}\n",
    "\n",
    "# define keys and values for rr headers mapping dictionary\n",
    "rr_keys = ['data_date', 'account_number', 'cash_collections', 'eir', 'default_date', 'recovery_date']\n",
    "rr_values = ['DATA_DATE', 'ACCOUNT_NUMBER', 'CASH_COLLECTIONS', 'EIR', 'DEFAULT_DATE', 'RECOVERY_DATE']\n",
    "# Map the column headers (values) of the dataframe to the corresponding keys\n",
    "rr_column_headers_dict = {k: v for k, v in zip(rr_keys, rr_values)}\n",
    "\n",
    "# declare valuation date\n",
    "valuation_date = datetime(year=2022, month=12, day=31).date()\n",
    "\n",
    "# Declare transition periodicity\n",
    "periodicity = 'Quarterly'.casefold() # can declare 'Annual', 'Quarterly' and 'Monthly' as applicable\n",
    "\n",
    "# Declare staging criterion: dpd or risk status\n",
    "staging_criterion = 'risk status' # alternative: 'days past due'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATA PRE-PROCESSING\n",
    "Perform data pre-processing to ensure data is in the desired format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pd_data_prep(pd_data, periodicity, staging_criterion, data_date, client_id, account_number, loan_segment, eir, risk_status, out_bal, days_past_due):\n",
    "    \"\"\"This function takes the columns of a dataframe as its arguments and perfoms cleanup and preprocessing of PD data\"\"\"\n",
    "\n",
    "    origin = np.datetime64('1900-01-01', 'D')\n",
    "\n",
    "    # convert date column to datetime\n",
    "    pd_data[data_date] = pd_data[data_date] - 2\n",
    "    pd_data[data_date] = origin + np.array(pd_data[data_date], dtype= 'timedelta64[D]')\n",
    "\n",
    "    # sort values based on date\n",
    "    pd_data = pd_data.sort_values(by = [data_date])\n",
    "\n",
    "    # convert client_id to str\n",
    "    pd_data[client_id] = pd_data[client_id].astype(str)\n",
    "    pd_data[client_id] = pd_data[client_id] #.str.replace(' ', '')\n",
    "    \n",
    "    # convert account number to str\n",
    "    pd_data[account_number] = pd_data[account_number].astype(str).str.replace(\"'\", '')\n",
    "    pd_data[account_number] = pd_data[account_number].astype(str)\n",
    "\n",
    "    # convert loan_segment to str\n",
    "    pd_data[loan_segment] = pd_data[loan_segment].astype(str)\n",
    "    pd_data[loan_segment] = pd_data[loan_segment].str.lower()\n",
    "\n",
    "    # convert eir to float\n",
    "    pd_data[eir] = pd_data[eir].astype(str).str.replace(' ', '')\n",
    "    pd_data[eir] = pd_data[eir].astype(float).fillna(0.0)\n",
    "\n",
    "    # convert risk_status to str\n",
    "    pd_data[risk_status] = pd_data[risk_status].astype(str)\n",
    "\n",
    "    # convert out_bal to float\n",
    "    pd_data[out_bal] = pd_data[out_bal].astype(str).str.replace('(', '-').str.replace(r'[^0-9.-]', '', regex = True)  \n",
    "    # pd_data[out_bal] = pd_data[out_bal].astype(str).str.replace(r'[^0-9.-]', '', regex = True)    \n",
    "    pd_data[out_bal] = pd.to_numeric(pd_data[out_bal], errors = 'coerce').fillna(0.0)\n",
    "\n",
    "    # convert days past due to numeric\n",
    "    pd_data[days_past_due] = pd_data[days_past_due].astype(str).str.replace(r'[^0-9.-]', '', regex = True)    \n",
    "    pd_data[days_past_due] = pd.to_numeric(pd_data[days_past_due], errors = 'coerce').fillna(0.0)\n",
    "\n",
    "    # filter negative outstanding balances\n",
    "    pd_data = pd_data[pd_data[out_bal] >= 0]\n",
    "\n",
    "    # create unique identifier for current status\n",
    "    pd_data['ID_NOW'] = pd_data.apply(lambda row: row[data_date].strftime('%d-%m-%Y') + '_' + row[account_number], axis = 1)\n",
    "\n",
    "    # create unique identifier for next status determined by periodicity\n",
    "    \n",
    "    if periodicity == 'Annual'.casefold():\n",
    "        shift_value = 13\n",
    "    elif periodicity == 'Quarterly'.casefold():\n",
    "        shift_value = 4\n",
    "    elif periodicity == 'Monthly'.casefold():\n",
    "        shift_value = 2\n",
    "       \n",
    "    pd_data['SHIFTED_DATE'] = (pd_data[data_date].values.astype('M8[M]') + np.timedelta64(shift_value, 'M')).astype('M8[D]') - np.timedelta64(1, 'D')\n",
    "\n",
    "    # create ID_NEXT column for next status\n",
    "    pd_data['ID_NEXT'] = pd_data.apply(lambda row: row['SHIFTED_DATE'].strftime('%d-%m-%Y') + '_' + row[account_number], axis = 1)\n",
    "\n",
    "    # map dpd to current stage\n",
    "    def dpd_staging_map(days):\n",
    "        \"\"\"This function returns the IFRS 9 stage based on days past due\"\"\"\n",
    "        if days <= 30:\n",
    "            return 'Stage 1'\n",
    "        elif days <= 90:\n",
    "            return 'Stage 2'\n",
    "        else:\n",
    "            return 'Stage 3'\n",
    "\n",
    "    # map risk status to stage current\n",
    "    def risk_status_staging_map(status):\n",
    "        \"\"\"This function maps the risk status to equivalent IFRS 9 stages\"\"\"\n",
    "        if status.casefold() == 'normal'.casefold():\n",
    "            return 'Stage 1'\n",
    "        elif status.casefold() == 'watch'.casefold():\n",
    "            return 'Stage 2'\n",
    "        else:\n",
    "            return 'Stage 3'\n",
    "        \n",
    "    # create stage now column based on selected staging_criterion\n",
    "    if staging_criterion == 'risk status'.casefold():\n",
    "        pd_data['STAGE_NOW'] = pd_data[risk_status].apply(risk_status_staging_map)\n",
    "\n",
    "    elif staging_criterion == 'days past due'.casefold():\n",
    "        pd_data['STAGE_NOW'] = pd_data[days_past_due].apply(dpd_staging_map)\n",
    "\n",
    "    # map dpd to next stage\n",
    "        # create dictionary with ID_NOW as keys and stage now as values\n",
    "    lookup_id = pd_data.set_index('ID_NOW')['STAGE_NOW'].to_dict()\n",
    "\n",
    "    # create stage next by checking mapping dictionary using get() method on ID_NEXT and returning from stage now; return 'Exit' if not found\n",
    "    pd_data['STAGE_NEXT'] = pd_data['ID_NEXT'].apply(lambda x: lookup_id.get(x, 'Exit'))   \n",
    "    \n",
    "    # define conditions for curing\n",
    "    conditions = (pd_data['STAGE_NOW'] == 'Stage 3') & ((pd_data['STAGE_NEXT'] == 'Stage 2') | (pd_data['STAGE_NEXT'] == 'Stage 1'))\n",
    "\n",
    "    # Compare stage now and stage next to map cures based on conditions. \n",
    "    pd_data['CURES'] = np.where(conditions, 'Cured', pd_data['STAGE_NEXT'])\n",
    "    # pd_data[\"CURED\"] = pd_data.apply(lambda x: cures(stage_now=x['STAGE_NOW'], stage_next=x[\"STAGE_NEXT\"]), axis = 1)\n",
    "\n",
    "    cleaned_pd_data = pd_data\n",
    "\n",
    "    return cleaned_pd_data\n",
    "\n",
    "pd_prep_output = pd_data_prep(pd_data, periodicity, staging_criterion, *pd_column_headers_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function that takes in recoveries data and pre-processes it\n",
    "def recoveries_data_prep(cleaned_pd_data, rr_data, periodicity, data_date, account_number, cash_collections, eir, default_date, recovery_date):\n",
    "    \"\"\"This function takes the recoveries data and pre-processes it.\"\"\"\n",
    "\n",
    "    origin = np.datetime64('1900-01-01', 'D')\n",
    "\n",
    "    # ensure all date columns are datetime objects\n",
    "    rr_data[data_date] = rr_data[data_date].fillna(42370)\n",
    "    rr_data[data_date] = rr_data[data_date] - 2\n",
    "    rr_data[data_date] = origin + np.array(rr_data[data_date], dtype='timedelta64[D]')\n",
    "\n",
    "    rr_data[default_date] = rr_data[default_date] - 2\n",
    "    rr_data[default_date] = origin + np.array(rr_data[default_date], dtype='timedelta64[D]')\n",
    "\n",
    "    rr_data[recovery_date] = rr_data[recovery_date].fillna(rr_data[data_date])\n",
    "    rr_data[recovery_date] = rr_data[recovery_date] - 2\n",
    "    rr_data[recovery_date] = origin + np.array(rr_data[recovery_date], dtype='timedelta64[D]')\n",
    "\n",
    "    # clean account number\n",
    "    rr_data[account_number] = rr_data[account_number].astype(str).str.replace(\"'\",'')\n",
    "    rr_data[account_number] = rr_data[account_number].astype(str)\n",
    "\n",
    "    # set cash collections to numeric\n",
    "    rr_data[cash_collections] = rr_data[cash_collections].astype(str).str.replace('(', '-').str.replace(r'[^0-9.-]', '', regex = True)\n",
    "    # rr_data[cash_collections] = rr_data[cash_collections].astype(str).str.replace(r'[^0-9.-]', '', regex = True)\n",
    "    rr_data[cash_collections] = pd.to_numeric(rr_data[cash_collections], errors= 'coerce').fillna(0.0)\n",
    "\n",
    "    # set eir to numeric\n",
    "    rr_data[eir] = rr_data[eir]\n",
    "    rr_data[eir] = rr_data[eir].astype(float).fillna(0.0)\n",
    "\n",
    "    # filter negative recoveries\n",
    "    rr_data = rr_data[rr_data[cash_collections] >= 0]    \n",
    "\n",
    "    # create unique identifier for current status\n",
    "    rr_data['ID_NOW'] = rr_data.apply(lambda row: row[data_date].strftime('%d-%m-%Y') + '_' + row[account_number], axis = 1)\n",
    "\n",
    "    # create unique identifier for next status determined by periodicity\n",
    "    if periodicity == 'Annual'.casefold():\n",
    "        shift_value = 13\n",
    "    elif periodicity == 'Quarterly'.casefold():\n",
    "        shift_value = 4\n",
    "    elif periodicity == 'Monthly'.casefold():\n",
    "        shift_value = 2\n",
    "       \n",
    "    rr_data['SHIFTED_DATE'] = (rr_data[data_date].values.astype('M8[M]') + np.timedelta64(shift_value, 'M')).astype('M8[D]') - np.timedelta64(1, 'D')\n",
    "\n",
    "    # create ID_NEXT column for next status\n",
    "    rr_data['ID_NEXT'] = rr_data.apply(lambda row: row['SHIFTED_DATE'].strftime('%d-%m-%Y') + '_' + row[account_number], axis = 1)\n",
    "    \n",
    "    # map the account number to the first recorded instance of default occuring when there is no default date\n",
    "    mapped_default_date = cleaned_pd_data.loc[((cleaned_pd_data['STAGE_NOW']\n",
    "                                                .isin(['Stage 1', 'Stage 2'])) & (cleaned_pd_data['STAGE_NEXT'] == 'Stage 3'))\n",
    "                                                ].groupby('ACCOUNT_NUMBER')['DATA_DATE'].first().reset_index()\n",
    "    \n",
    "    mapped_default_date.columns = ['ACCOUNT_NUMBER', 'MAPPED_DEFAULT_DATE']\n",
    "\n",
    "    # merge mapped default date to rr_data\n",
    "    rr_data = pd.merge(rr_data, mapped_default_date, on = 'ACCOUNT_NUMBER', how = 'left')\n",
    "\n",
    "    # replace NaN values in mapped default date\n",
    "    rr_data['MAPPED_DEFAULT_DATE'] = rr_data['MAPPED_DEFAULT_DATE'].fillna(rr_data[recovery_date] - pd.offsets.DateOffset(months = 3))\n",
    "\n",
    "    # determine the final default date to use\n",
    "    rr_data['FINAL_DEFAULT_DATE'] = rr_data[default_date].fillna(rr_data['MAPPED_DEFAULT_DATE'])\n",
    "\n",
    "    # determine the time in default\n",
    "    rr_data['TIME IN DEFAULT'] = rr_data[recovery_date] - rr_data['FINAL_DEFAULT_DATE']\n",
    "    rr_data[\"TIME IN DEFAULT\"] = rr_data['TIME IN DEFAULT'].apply(lambda x: (x.days)/365)\n",
    "\n",
    "    # discount the cash collections \n",
    "    rr_data['DISCOUNTED_RECOVERIES'] = rr_data[cash_collections]*((1.0 + rr_data[eir])**(-rr_data['TIME IN DEFAULT']))\n",
    "\n",
    "    # group duplicates into final discounted recoveries dataframe\n",
    "    discounted_recoveries = rr_data.groupby('ID_NOW')['DISCOUNTED_RECOVERIES'].sum().reset_index()\n",
    "\n",
    "    # merge the discounted recoveries to the cleaned pd data\n",
    "    pd_rr_data = pd.merge(cleaned_pd_data, discounted_recoveries, on = 'ID_NOW', how = 'left')\n",
    "\n",
    "    # create conditions on which to create the recoveries status column\n",
    "    conditions = (pd_rr_data['STAGE_NOW'] == 'Stage 3') & (pd_rr_data['CURES'] != 'Cured') & (pd_rr_data['DISCOUNTED_RECOVERIES'] > 0)\n",
    "\n",
    "    pd_rr_data['RECOVERIES'] = np.where(conditions, 'Recovered', pd_rr_data['CURES'])\n",
    "\n",
    "    # rename rr_data to cleaned_rr_data\n",
    "    cleaned_rr_data = rr_data\n",
    "\n",
    "    # subset pd_rr_data based on each segment and add to dictionary\n",
    "    final_data_dict = {}\n",
    "    for segment in pd_rr_data['LOAN_SEGMENT'].unique():\n",
    "        segment_df = pd_rr_data[pd_rr_data['LOAN_SEGMENT'] == segment]\n",
    "        final_data_dict[segment] = segment_df\n",
    "\n",
    "    return pd_rr_data, cleaned_rr_data, final_data_dict\n",
    "\n",
    "recoveries_prep_output = recoveries_data_prep(pd_prep_output, recoveries_data, periodicity, *rr_column_headers_dict.values())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRANSITION MATRICES\n",
    "Construct transition matrices;\n",
    "Perform matrix multiplication based on periodicity;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function that constructs transtition matrices \n",
    "def construct_matrices(pd_rr_data, periodicity, valuation_date):\n",
    "    \"\"\"This fun,ction creates transition matrices for each segment of loans and assigns the projected 'transition date' for the nth matrix from the valuation date\"\"\"\n",
    "\n",
    "    # Create list of dates and insert valuation date\n",
    "    date_projections = [valuation_date]\n",
    "    \n",
    "     # Declare n - number of transitions for the model\n",
    "        # add dates to list based on transition criteria (quarterly/monthly/annualy)\n",
    "    if periodicity == 'Annual'.casefold():\n",
    "        n = 30\n",
    "        shift_value = 12\n",
    "    elif periodicity == 'Quarterly'.casefold():\n",
    "        n = 45\n",
    "        shift_value = 3\n",
    "    elif periodicity == 'Monthly'.casefold():\n",
    "        n = 120\n",
    "        shift_value = 1\n",
    "        \n",
    "    for i in range(n):\n",
    "        next_date = date_projections[i] + relativedelta(months = shift_value, day = 31)\n",
    "        date_projections.append(next_date)        \n",
    " \n",
    "    # define all possible stages of loans\n",
    "    all_stages_pds = ['Stage 1', 'Stage 2', 'Stage 3']\n",
    "    all_stages_rr = ['Cured', 'Recovered', 'Stage 3']\n",
    "    base_matrices_dict = {}\n",
    "    rr_matrices_dict = {}\n",
    "\n",
    "    # ______________________________________________________________________________________________________________________________________________________________\n",
    "\n",
    "    # segmentation for pds\n",
    "    for segment in pd_rr_data['LOAN_SEGMENT'].unique():\n",
    "        # filter the dataframe to obtain a df for each unique segment\n",
    "        segment_pd_data = pd_rr_data[pd_rr_data['LOAN_SEGMENT'] == segment]\n",
    "\n",
    "        # filter out exits from the population - implicit assumption: exits occur due to reasons other than clearing the balance\n",
    "        segment_pd_data = segment_pd_data[segment_pd_data['STAGE_NEXT'] != 'Exit']\n",
    "\n",
    "        # construct a base transition matrix from the filtered dataframe\n",
    "        base_transition_matrix = pd.pivot_table(data= segment_pd_data, values= 'OUT_BAL', index= 'STAGE_NOW', columns= 'STAGE_NEXT', aggfunc='sum', fill_value=0)\n",
    "\n",
    "        # reindex to ensure all stages present\n",
    "        base_transition_matrix = base_transition_matrix.reindex(index= all_stages_pds, columns= all_stages_pds).fillna(0)\n",
    "\n",
    "        # set stage 3 row to 0 - Assumption: No transitions out of default\n",
    "        base_transition_matrix.loc['Stage 3'] = 0\n",
    "\n",
    "        # set stage3-stage3 to 1 - absorbing state created\n",
    "        base_transition_matrix.at['Stage 3', 'Stage 3'] = 1\n",
    "\n",
    "        # calculate row sums\n",
    "        row_sums = base_transition_matrix.sum(axis = 1)\n",
    "\n",
    "        # convert each entry into a percentage of the row total by dividing by row_sums\n",
    "        base_transition_matrix = base_transition_matrix.div(row_sums, axis = 0)\n",
    "        \n",
    "        # Add base transition matrix to base_matrices dictionary \n",
    "        base_matrices_dict[segment] = base_transition_matrix\n",
    "\n",
    "    # Perform matrix multiplication to obtain the future transition matrices\n",
    "    pd_matrix_dict = {} # Define dictionary to store matrices\n",
    "\n",
    "    # create transition matrices for pd projections\n",
    "    for segment in base_matrices_dict:\n",
    "        # create a list to store nth transition matrices: initialize by storing base matrix.\n",
    "        base_matrix = base_matrices_dict[segment]\n",
    "        pd_transition_matrices = [base_matrix]\n",
    "\n",
    "        # multiply matrices\n",
    "        for _ in range(n):\n",
    "            previous_matrix = pd_transition_matrices[-1]\n",
    "            next_matrix = previous_matrix @ base_matrix\n",
    "            pd_transition_matrices.append(next_matrix)\n",
    "\n",
    "    # map the nth date and nth matrix to each other and store in date-matrix pairing for each segment\n",
    "        date_pd_matrix_pairing = {d: m for d, m in zip(date_projections, pd_transition_matrices)}\n",
    "\n",
    "    # map the segment to the date matrix pairing to create a nested dictionary \n",
    "        pd_matrix_dict[segment] = date_pd_matrix_pairing\n",
    "\n",
    "    # _______________________________________________________________________________________________________________________________________________________________\n",
    "\n",
    "    # segmentation for cures and recoveries\n",
    "    for segment in pd_rr_data['LOAN_SEGMENT'].unique():\n",
    "        # filter the dataframe to obtain a df for each unique segment\n",
    "        segment_rr_data = pd_rr_data[pd_rr_data['LOAN_SEGMENT'] == segment]\n",
    "\n",
    "        # filter out exits from the population - implicit assumption: exits occur due to reasons other than clearing the balance\n",
    "        segment_rr_data = segment_rr_data[segment_rr_data['RECOVERIES'] != 'Exit']\n",
    "\n",
    "        # construct a cure transition matrix from the filtered dataframe\n",
    "        rr_transition_matrix = pd.pivot_table(data= segment_rr_data, values= 'OUT_BAL', index= 'STAGE_NOW', columns= 'RECOVERIES', aggfunc='sum', fill_value=0)\n",
    "\n",
    "        # reindex to ensure all stages present\n",
    "        rr_transition_matrix = rr_transition_matrix.reindex(index= all_stages_rr, columns= all_stages_rr).fillna(0)\n",
    "\n",
    "        # set cure-cure and rr-rr to 1\n",
    "        rr_transition_matrix.at['Cured', 'Cured'] = 1\n",
    "        rr_transition_matrix.at['Recovered', 'Recovered'] = 1\n",
    "\n",
    "        # calculate row sums\n",
    "        row_sums = rr_transition_matrix.sum(axis = 1)\n",
    "\n",
    "        # convert each entry into a percentage of the row total by dividing by row_sums\n",
    "        rr_transition_matrix = rr_transition_matrix.div(row_sums, axis = 0)\n",
    "        \n",
    "        # Add base transition matrix to base_matrices dictionary \n",
    "        rr_matrices_dict[segment] = rr_transition_matrix\n",
    "\n",
    "    # create cr_rr matrix dictionary to store cr_rr matrices\n",
    "    rr_matrix_dict = {}\n",
    "       \n",
    "    # create transition matrices for cr-rr projections\n",
    "    for segment in rr_matrices_dict:\n",
    "        # create a dixctionary to store the nth transition matrices: starting with the base rr matrix\n",
    "        base_rr_matrix = rr_matrices_dict[segment]\n",
    "        rr_transition_matrices = [base_rr_matrix]\n",
    "\n",
    "        # multiply matrices\n",
    "        for _ in range(n):\n",
    "            previous_rr_matrix = rr_transition_matrices[-1]\n",
    "            next_rr_matrix = previous_rr_matrix @ base_rr_matrix\n",
    "            rr_transition_matrices.append(next_rr_matrix)\n",
    "\n",
    "        # create a dictionary to pair the nth rr_transition matrix to the nth date\n",
    "        date_rr_matrix_pairing = {d: m for d, m in zip(date_projections, rr_transition_matrices)}\n",
    "\n",
    "        # store the rr_date pairings and the relevant segment in rr_matrix_dict\n",
    "        rr_matrix_dict[segment] = date_rr_matrix_pairing\n",
    "    \n",
    "    return pd_matrix_dict, rr_matrix_dict\n",
    "\n",
    "construct_matrices_output = construct_matrices(recoveries_prep_output[0], periodicity, valuation_date) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXTRACT DEFAULT PROBABILITIES\n",
    "Extract the PDs from the transition matrices for each loan segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to extract PDs, Cures rates and Recovery Rates from transition matrices\n",
    "def extract_rates(pd_matrix_dict, rr_matrix_dict):\n",
    "    \"\"\"This function extracts the unconditional pds, cure rates and recovery rates from the input matrix dictionaries and outputs a dataframe of both\n",
    "    unconditional and conditional rates.\"\"\"\n",
    "\n",
    "    # extract the pds\n",
    "    segment_pds = []\n",
    "\n",
    "    # loop through each matrix in matrices for each date and for each segment\n",
    "    for segment, matrices in pd_matrix_dict.items():\n",
    "        # initialize empty lists to store PDs for current segment\n",
    "        pd_dates = []\n",
    "        stage_1_cml_pds = []\n",
    "        stage_2_cml_pds = []\n",
    "\n",
    "        for date_index, matrix in matrices.items():\n",
    "            # Access desired element of matrix\n",
    "            pd_1 = matrix.iloc[0,2]\n",
    "            pd_2 = matrix.iloc[1,2]\n",
    "\n",
    "            # append each date to dates and each matrix element to its corresponding list\n",
    "            pd_dates.append(date_index)\n",
    "            stage_1_cml_pds.append(pd_1)\n",
    "            stage_2_cml_pds.append(pd_2)\n",
    "\n",
    "        # add cumulative pds lists to list\n",
    "        cml_pds_list = [stage_1_cml_pds, stage_2_cml_pds]\n",
    "\n",
    "        # create output list\n",
    "        cond_pds_list = []\n",
    "\n",
    "        # compute the conditional pds from the cumulative pds\n",
    "        for pds_sublist in cml_pds_list:\n",
    "            pd_output = [pds_sublist[i] if i == 0 else ((pds_sublist[i]-pds_sublist[i-1])/(1-pds_sublist[i-1])) for i in range(0, len(pds_sublist))]\n",
    "            cond_pds_list.append(pd_output)\n",
    "\n",
    "        # create dictionary of PDs and Dates\n",
    "        cml_pds_dict = {\n",
    "            'Segment': [segment] * len(pd_dates),\n",
    "            'Date': pd_dates,\n",
    "            'Stage 1 cml. PDs': stage_1_cml_pds,\n",
    "            'Stage 2 cml. PDs': stage_2_cml_pds,\n",
    "            'Stage 1 cond. PDs': cond_pds_list[0],\n",
    "            'Stage 2 cond. PDs': cond_pds_list[1],\n",
    "        }\n",
    "\n",
    "        # create pds dataframe \n",
    "        cml_pds = pd.DataFrame(cml_pds_dict)\n",
    "\n",
    "        # Add segment dataframe to the list of dataframes\n",
    "        segment_pds.append(cml_pds)\n",
    "\n",
    "    # create empty dataframe for the pds\n",
    "    all_pds = pd.DataFrame()\n",
    "\n",
    "    for segment_pd in segment_pds:\n",
    "        # Add blank row\n",
    "        # blank_row = pd.DataFrame([[''] * segment_pd.shape[1]], columns= segment_pd.columns)\n",
    "        all_pds = pd.concat([all_pds, segment_pd], ignore_index= True)\n",
    "\n",
    "    # ________________________________________________________________________________________________________________________________________________________________\n",
    "\n",
    "    # extract cures and recoveries\n",
    "        # extract the pds\n",
    "    segment_rrs = []\n",
    "\n",
    "    # loop through each matrix in matrices for each date and for each segment\n",
    "    for segment, matrices in rr_matrix_dict.items():\n",
    "        # initialize empty lists to store PDs for current segment\n",
    "        rr_dates = []\n",
    "        cml_cr = []\n",
    "        cml_rr = []\n",
    "\n",
    "        for date_index, matrix in matrices.items():\n",
    "            # Access desired element of matrix\n",
    "            cr = matrix.iloc[2,0]\n",
    "            rr = matrix.iloc[2,1]\n",
    "\n",
    "            # append each date to dates and each matrix element to its corresponding list\n",
    "            rr_dates.append(date_index)\n",
    "            cml_cr.append(cr)\n",
    "            cml_rr.append(rr)\n",
    "\n",
    "        # add cumulative pds lists in list\n",
    "        cml_rr_list = [cml_cr, cml_rr]\n",
    "\n",
    "        # create output list\n",
    "        cond_rr_list = []\n",
    "\n",
    "        # compute the conditional pds from the cumulative pds\n",
    "        for sublist in cml_rr_list:\n",
    "            rr_output = [sublist[i] if i == 0 else ((sublist[i]-sublist[i-1])/(1-sublist[i-1])) for i in range(0, len(sublist))]\n",
    "            cond_rr_list.append(rr_output)\n",
    "\n",
    "        # create dictionary of PDs and Dates\n",
    "        cml_rr_dict = {\n",
    "            'Segment': [segment] * len(rr_dates),\n",
    "            'Date': rr_dates,\n",
    "            'Cml. CRs': cml_cr,\n",
    "            'Cml. RRs': cml_rr,\n",
    "            'Cond. CRs': cond_rr_list[0],\n",
    "            'Cond. RRs': cond_rr_list[1],\n",
    "        }\n",
    "\n",
    "        # create pds dataframe \n",
    "        cml_rr = pd.DataFrame(cml_rr_dict)\n",
    "\n",
    "        # Add segment dataframe to the list of dataframes\n",
    "        segment_rrs.append(cml_rr)\n",
    "\n",
    "    # create empty dataframe for the recoveries\n",
    "    all_rrs = pd.DataFrame()\n",
    "\n",
    "    for segment_rr in segment_rrs:\n",
    "        # Add blank row\n",
    "        # blank_row = pd.DataFrame([[''] * segment_rr.shape[1]], columns= segment_rr.columns)\n",
    "        all_rrs = pd.concat([all_rrs, segment_rr], ignore_index= True)\n",
    "\n",
    "    all_pd_rr_merge = pd.merge(all_pds, all_rrs, on = ['Segment', 'Date'], how = 'left')\n",
    "    \n",
    "    all_pd_rr = pd.DataFrame()\n",
    "\n",
    "    for segment in all_pd_rr_merge['Segment'].unique():\n",
    "        segment_df = all_pd_rr_merge[all_pd_rr_merge['Segment'] == segment]\n",
    "        all_pd_rr = pd.concat([all_pd_rr, segment_df, pd.DataFrame([[]])], ignore_index= True)\n",
    "\n",
    "\n",
    "    return all_pd_rr, all_pds, all_rrs, all_pd_rr_merge\n",
    "\n",
    "rates_output = extract_rates(construct_matrices_output[0], construct_matrices_output[1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXPORT RESULTS TO .CSV\n",
    "Export Transition Matrices; Export Cleaned & Transformed PD Data; Export PD Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export transition matrices for each segment\n",
    "for i, item in enumerate(construct_matrices_output):\n",
    "    for segment in item.keys():\n",
    "        output = pd.DataFrame([item[segment]])\n",
    "        segment_name = segment.replace('/', ' ')\n",
    "        if i == 0:\n",
    "            output.to_csv(path_write + '/Transition Matrices/' + 'np_pd_matrices' + '_' + segment_name + '.csv', index = False)\n",
    "        elif i == 1:\n",
    "            output.to_csv(path_write + '/Transition Matrices/' + 'np_cr_rr_matrices' + '_' + segment_name + '.csv', index = False)\n",
    "\n",
    "print('Successfully Exported All Transition Tatrices')\n",
    "\n",
    "# Export pds and rrs to csv\n",
    "rates_output[0].to_csv(path_write + 'example PDs_RRs.csv', index = False)\n",
    "print(\"Successfully Exported Non-FLI-Adjusted PDs!\")\n",
    "\n",
    "# Export cleaned pd_rr data to csv\n",
    "recoveries_prep_output[0].to_csv(path_write + 'example_cleaned Data.csv', index = False)\n",
    "print(\"Successfully Exported Cleaned PD Data!\")\n",
    "\n",
    "# Export cleaned recoveries data\n",
    "recoveries_prep_output[1].to_csv(path_write + 'example cleaned recoveries Data.csv', index = False)\n",
    "print(\"Successfully Exported Pre-Processed Recoveries Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to compute the NPLs for each loan segment in the data\n",
    "def historical_segment_npl(cleaned_pd_data, periodicity):\n",
    "    \"\"\"This function takes in the historical PD data and periodicity and returns a DataFrame with the historical NPL rates for each unique segment computed.\"\"\"\n",
    "\n",
    "    # set npl frequency based on periodicity\n",
    "    if periodicity == 'Annual'.casefold():\n",
    "        frequency = 'A'\n",
    "    elif periodicity == 'Quarterly'.casefold():\n",
    "        frequency = 'Q'\n",
    "    elif periodicity == 'Monthly'.casefold():\n",
    "        frequency = 'M'\n",
    "\n",
    "    # initialize empty lists to store NPLs\n",
    "    all_npls = []\n",
    "    # npls_spaced = pd.DataFrame()\n",
    "\n",
    "    # for each segment compute the NPLs\n",
    "    for segment in cleaned_pd_data['LOAN_SEGMENT'].unique():\n",
    "        # initialize empty list to store npls computed for each 'frequency'\n",
    "        results = []\n",
    "        # subset data by unique segment\n",
    "        df = cleaned_pd_data[cleaned_pd_data['LOAN_SEGMENT'] == segment]\n",
    "        # group subset data by date at the required frequency level\n",
    "        grouped_df = df.groupby(pd.Grouper(key = 'DATA_DATE', freq = frequency))\n",
    "        # for each group (e.g., month, quarter, year) compute the NPL rate and store in result dictionary\n",
    "        for group_key, group in grouped_df:\n",
    "            stage1_stage3 = group[(group['STAGE_NOW'] == 'Stage 1') & (group['STAGE_NEXT'] == 'Stage 3')]\n",
    "            stage1 = group[group['STAGE_NOW'] == 'Stage 1']\n",
    "            stage1_exit = group[(group['STAGE_NOW'] == 'Stage 1') & (group['STAGE_NEXT'] == 'Exit')]\n",
    "\n",
    "            x = stage1_stage3['OUT_BAL'].sum()\n",
    "            y = stage1['OUT_BAL'].sum()\n",
    "            z = stage1_exit['OUT_BAL'].sum()\n",
    "\n",
    "            s1_npl_ratio = x/(y-z)\n",
    "\n",
    "            stage2_stage3 = group[(group['STAGE_NOW'] == 'Stage 2') & (group['STAGE_NEXT'] == 'Stage 3')]\n",
    "            stage2 = group[group['STAGE_NOW'] == 'Stage 2']\n",
    "            stage2_exit = group[(group['STAGE_NOW'] == 'Stage 2') & (group['STAGE_NEXT'] == 'Exit')]\n",
    "\n",
    "            a = stage2_stage3['OUT_BAL'].sum()\n",
    "            b = stage2['OUT_BAL'].sum()\n",
    "            c = stage2_exit['OUT_BAL'].sum()\n",
    "\n",
    "            s2_npl_ratio = a/(b-c)           \n",
    "\n",
    "            result = {\n",
    "                'Segment': segment,\n",
    "                f'{periodicity.capitalize()} Period': group_key,\n",
    "                'Stage 1 NPL Ratio': s1_npl_ratio,\n",
    "                'Stage 2 NPL Ratio': s2_npl_ratio,\n",
    "            }\n",
    "            # append result dictionary to results list to obtain list of all npls for the segment\n",
    "            results.append(result)\n",
    "        # convert the segment results list to a dataframe\n",
    "        segment_npl = pd.DataFrame(results)\n",
    "        # append the segment results list to the all_npls list\n",
    "        all_npls.append(segment_npl)\n",
    "\n",
    "    npls_spaced = []\n",
    "    # convert the all_npls list to a dataframe and add a space between each item\n",
    "    for df in all_npls:\n",
    "        npls_spaced.append(df)\n",
    "        blank_row = pd.DataFrame([[]])\n",
    "        npls_spaced.append(blank_row)\n",
    "    \n",
    "    npls_spaced = pd.concat(npls_spaced, ignore_index= True).reset_index(drop= True)\n",
    "\n",
    "    return npls_spaced\n",
    "\n",
    "segment_npls = historical_segment_npl(recoveries_prep_output[0], periodicity)\n",
    "segment_npls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to compute the NPLs for each loan segment in the data\n",
    "def historical_total_npl(cleaned_pd_data, periodicity):\n",
    "    \"\"\"This function takes in the historical PD data and periodicity and returns a DataFrame with the historical NPL rates for each unique segment computed.\"\"\"\n",
    "\n",
    "    # set npl frequency based on periodicity\n",
    "    if periodicity == 'Annual'.casefold():\n",
    "        frequency = 'A'\n",
    "    elif periodicity == 'Quarterly'.casefold():\n",
    "        frequency = 'Q'\n",
    "    elif periodicity == 'Monthly'.casefold():\n",
    "        frequency = 'M'\n",
    "\n",
    "    # initialize empty lists to store NPLs\n",
    "    all_npls = []\n",
    "\n",
    "    # group cleaned PD Data by date based on frequency\n",
    "    grouped_df = cleaned_pd_data.groupby(pd.Grouper(key = 'DATA_DATE', freq = frequency))\n",
    "    # for each group (e.g., month, quarter, year) compute the NPL rate and store in result dictionary\n",
    "    for group_key, group in grouped_df:\n",
    "        stage1_stage3 = group[(group['STAGE_NOW'] == 'Stage 1') & (group['STAGE_NEXT'] == 'Stage 3')]\n",
    "        stage1 = group[group['STAGE_NOW'] == 'Stage 1']\n",
    "        stage1_exit = group[(group['STAGE_NOW'] == 'Stage 1') & (group['STAGE_NEXT'] == 'Exit')]\n",
    "\n",
    "        x = stage1_stage3['OUT_BAL'].sum()\n",
    "        y = stage1['OUT_BAL'].sum()\n",
    "        z = stage1_exit['OUT_BAL'].sum()\n",
    "\n",
    "        s1_npl_ratio = x/(y-z)\n",
    "\n",
    "        stage2_stage3 = group[(group['STAGE_NOW'] == 'Stage 2') & (group['STAGE_NEXT'] == 'Stage 3')]\n",
    "        stage2 = group[group['STAGE_NOW'] == 'Stage 2']\n",
    "        stage2_exit = group[(group['STAGE_NOW'] == 'Stage 2') & (group['STAGE_NEXT'] == 'Exit')]\n",
    "\n",
    "        a = stage2_stage3['OUT_BAL'].sum()\n",
    "        b = stage2['OUT_BAL'].sum()\n",
    "        c = stage2_exit['OUT_BAL'].sum()\n",
    "\n",
    "        s2_npl_ratio = a/(b-c) \n",
    "\n",
    "        result = {\n",
    "            'Segment': 'Overall npl ratio',\n",
    "            f'{periodicity.capitalize()} Period': group_key,\n",
    "            'Stage 1 NPL Ratio': s1_npl_ratio,\n",
    "            'Stage 2 NPL Ratio': s2_npl_ratio,\n",
    "        }\n",
    "        # append result dictionary to results list to obtain list of all npls for the segment\n",
    "        all_npls.append(result)\n",
    "\n",
    "    # convert the results list to a dataframe\n",
    "    all_npls = pd.DataFrame(all_npls)\n",
    "\n",
    "    return all_npls\n",
    "\n",
    "all_npls = historical_total_npl(recoveries_prep_output[0], periodicity)\n",
    "all_npls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_npls.to_csv(path_write + 'example_segment_npls.csv')\n",
    "all_npls.to_csv(path_write + 'example_all_npls.csv')\n",
    "\n",
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duration = end_time-start_time\n",
    "print(f\"This took: {duration} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
